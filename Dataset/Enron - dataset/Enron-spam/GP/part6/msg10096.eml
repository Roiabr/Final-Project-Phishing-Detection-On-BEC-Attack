From: "Mail Delivery Subsystem" <MAILER-DAEMON>
To: <paliourg@iit.demokritos.gr>
Subject: Returned mail: see transcript for details
Date: Sat, 1 Jan 2005 16:18:40 +0300
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="----=_NextPart_000_184B_01C5B2FD.38349F90"
X-Priority: 3
X-MSMail-Priority: Normal
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2527

This is a multi-part message in MIME format.

------=_NextPart_000_184B_01C5B2FD.38349F90
Content-Type: text/html;
	charset="iso-8859-7"
Content-Transfer-Encoding: quoted-printable

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META HTTP-EQUIV=3D"Content-Type" CONTENT=3D"text/html; =
charset=3Diso-8859-7">
<META NAME=3D"Generator" CONTENT=3D"MS Exchange Server version =
6.5.7036.0">
<TITLE>Returned mail: see transcript for details</TITLE>
</HEAD>
<BODY>
<!-- Converted from text/plain format -->

<P><FONT SIZE=3D2>The original message was received at Mon, 27 Dec 2004 =
15:17:36 +0200 (EET)<BR>
from skel-users.iit.demokritos.gr [143.233.227.207]<BR>
<BR>
&nbsp;&nbsp; ----- The following addresses had permanent fatal errors =
-----<BR>
&lt;ernani@e-free.gr&gt;<BR>
<BR>
&nbsp;&nbsp; ----- Transcript of session follows -----<BR>
451 e-free.gr: Name server timeout<BR>
Message could not be delivered for 5 days<BR>
Message will be deleted from queue<BR>
</FONT>
</P>

</BODY>
</HTML>
------=_NextPart_000_184B_01C5B2FD.38349F90
Content-Type: text/plain;
	name="details.txt"
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename="details.txt"

Reporting-MTA: dns; iit.demokritos.gr
Arrival-Date: Mon, 27 Dec 2004 15:17:36 +0200 (EET)

Final-Recipient: RFC822; ernani@e-free.gr
Action: failed
Status: 4.4.7
Last-Attempt-Date: Sat, 1 Jan 2005 15:18:40 +0200 (EET)

------=_NextPart_000_184B_01C5B2FD.38349F90
Content-Type: message/rfc822
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment

From: "George Paliouras" <paliourg@iit.demokritos.gr>
To: "Ion Androutsopoulos" <ion@aueb.gr>
Cc: "Eirinaios Michelakis" <ernani@e-free.gr>,
	"Me" <paliourg@iit.demokritos.gr>
Subject: Re: MLJ submission MACH-2175-04: Learning to Filter Unsolicited Commercial E-Mail
Date: Mon, 27 Dec 2004 16:32:55 +0300
MIME-Version: 1.0
Content-Type: text/html;
	charset="iso-8859-7"
Content-Transfer-Encoding: quoted-printable
X-Priority: 3
X-MSMail-Priority: Normal
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.2527

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META HTTP-EQUIV=3D"Content-Type" CONTENT=3D"text/html; =
charset=3Diso-8859-7">
<META NAME=3D"Generator" CONTENT=3D"MS Exchange Server version =
6.5.7036.0">
<TITLE>Re: MLJ submission MACH-2175-04: Learning to Filter Unsolicited =
Commercial E-Mail</TITLE>
</HEAD>
<BODY>
<!-- Converted from text/plain format -->

<P><FONT SIZE=3D2>=BA=F9=ED,<BR>
<BR>
=CC=EF=F5 =F6=E1=DF=ED=E5=F4=E1=E9 =E1=F1=EA=E5=F4=DC =EA=E1=EB=DE =E7 =
=F0=F1=EF=F3=F0=DC=E8=E5=E9=E1 =EA=E1=F4=E1=E3=F1=E1=F6=DE=F2 =F4=E7=F2 =
=E4=EF=F5=EB=E5=E9=DC=F2 =F0=EF=F5 =DD=F7=EF=F5=EC=E5 =ED=E1<BR>
=EA=DC=ED=EF=F5=EC=E5. =D0=F1=DC=E3=EC=E1=F4=E9 =F5=F0=DC=F1=F7=E5=E9 =
=F0=DC=F1=E1 =F0=EF=EB=FD =E4=EF=F5=EB=E5=E9=DC. =C1=F5=F4=FC =F0=EF=F5 =
=E8=E1 =DE=E8=E5=EB=E1 =ED=E1 =E4=F9, =DF=F3=F9=F2 =E1=F0=FC<BR>
=F4=EF=ED =C5=E9=F1=E7=ED=E1=DF=EF, =E5=DF=ED=E1=E9 =EC=DF=E1 =
=E5=EA=F4=DF=EC=E7=F3=E7 =F4=EF=F5 =F7=F1=FC=ED=EF=F5 =F0=EF=F5 =
=F7=F1=E5=E9=DC=E6=E5=F4=E1=E9 =E3=E9=E1 =F4=EF =EA=DC=E8=E5 =F4=E9, =
=FE=F3=F4=E5<BR>
=ED=E1 =EC=F0=EF=F1=DD=F3=EF=F5=EC=E5 =ED=E1 =
=E5=EA=F4=E9=EC=DE=F3=EF=F5=EC=E5 =F4=E9 =E1=EE=DF=E6=E5=E9 =ED=E1 =
=EA=FC=F8=EF=F5=EC=E5 =E1=F0=FC =FC=EB=E1 =E1=F5=F4=DC. =
=D0=E9=F3=F4=E5=FD=F9 =FC=F4=E9 =F3=E5<BR>
=EF=F1=E9=F3=EC=DD=ED=E1 =F3=E7=EC=E5=DF=E1 =F5=F0=FC=F3=F7=E5=F3=E1=E9 =
=F0=E5=F1=E9=F3=F3=FC=F4=E5=F1=E1 =E1=F0=FC =FC=F3=E1 =EC=E1=F2 =
=E6=E7=F4=EF=FD=ED. =C5=F0=DF=F3=E7=F2, =E1=ED =
=F3=EA=EF=F0=DD=F5=E5=E9=F2<BR>
=ED=E1 =F4=EF =F3=F4=E5=DF=EB=E5=E9=F2 =E1=F5=F4=FC =F3=F4=EF=ED Tom, =
=E8=E1 =DE=E8=E5=EB=E1 =ED=E1 =EA=DC=ED=EF=F5=EC=E5 =
=EC=E5=F1=E9=EA=DD=F2 =E4=E9=EF=F1=E8=FE=F3=E5=E9=F2 =FE=F3=F4=E5 =ED=E1 =
=EC=E7=ED<BR>
=F5=F0=EF=F4=E9=EC=DC=EC=E5 =F4=E7 =E4=EF=F5=EB=E5=E9=DC =F0=EF=F5 =
=DD=F7=EF=F5=EC=E5 =EA=DC=ED=E5=E9 =EA=E1=E9 =ED=E1 =
=F5=F0=EF=F3=F7=E5=E8=EF=FD=EC=E5 =FC=F3=EF =F4=EF =E4=F5=ED=E1=F4=FC=ED =
=EB=E9=E3=FC=F4=E5=F1=E1<BR>
=F0=F1=DC=E3=EC=E1=F4=E1.<BR>
<BR>
=C5=F5=F7=DD=F2 =F0=EF=EB=EB=DD=F2 =E3=E9=E1 =F5=E3=E5=DF=E1 =EA=E1=E9 =
=E5=F5=F4=F5=F7=DF=E1.<BR>
<BR>
=C3=E9=FE=F1=E3=EF=F2<BR>
<BR>
Ion Androutsopoulos wrote:<BR>
<BR>
&gt; =D3=E1=F2 =F3=F4=DD=EB=ED=F9 =F4=EF =F0=EB=DC=ED=EF =F4=F9=ED =
=E1=EB=EB=E1=E3=FE=ED =E3=E9=E1 =F4=EF =DC=F1=E8=F1=EF =F4=EF=F5 MLJ. =
=D4=EF =DD=F7=F9 =E3=F1=DC=F8=E5=E9 =F3=E1=ED<BR>
&gt; =ED=E1 =E1=F0=E5=F5=E8=FD=ED=E5=F4=E1=E9 =F3=F4=EF=ED editor =
(=EA=E1=E9 =ED=EF=EC=DF=E6=F9 =F0=F9=F2 =E5=DF=ED=E1=E9 =EA=E1=EB=FC =
=ED=E1 =F4=EF=F5 =F4=EF<BR>
&gt; =F3=F4=E5=DF=EB=EF=F5=EC=E5 =E3=E9=E1 =ED=E1 =F4=EF=F5 =
=E4=E5=DF=EE=EF=F5=EC=E5 =F0=FC=F3=EF =F3=EF=E2=E1=F1=DC =
=DD=F7=EF=F5=EC=E5 =F0=DC=F1=E5=E9 =F4=E7=ED =F5=F0=FC=E8=E5=F3=E7) =
=E1=EB=EB=DC<BR>
&gt; =F0=E5=F1=E9=F3=F3=FC=F4=E5=F1=EF =E1=F0=E5=F5=E8=FD=ED=E5=F4=E1=E9 =
=F3=E5 =E5=EC=DC=F2. =C4=E5=DF=F4=E5 =F4=EF =F3=E1=ED =EC=E9=E1 =
=F0=F1=FE=F4=E7 =F0=F1=EF=F3=F0=DC=E8=E5=E9=E1 =ED=E1<BR>
&gt; =F3=EA=E5=F6=F4=EF=FD=EC=E5 =F3=EF=E2=E1=F1=DC =F4=E9 =
=E1=EA=F1=E9=E2=FE=F2 =E8=E1 =EA=DC=ED=EF=F5=EC=E5. =CC=E9=EB=DC=EC=E5 =
=F0=F1=EF=F6=E1=ED=FE=F2 =E3=E9=E1 =F0=EF=EB=EB=DE<BR>
&gt; =E4=EF=F5=EB=E5=E9=DC. =C8=E1 =F0=F1=EF=F3=F0=E1=E8=DE=F3=F9 =ED=E1 =
=E2=F1=F9 =DD=ED=E1=ED =F6=EF=E9=F4=E7=F4=DE =F0=EF=F5 =E8=E1 =
=E1=F3=F7=EF=EB=E7=E8=E5=DF =EC=E5 =FC=EB=E1 =E1=F5=F4=DC<BR>
&gt; =E3=E9=E1 =F4=E7=ED =F0=F4=F5=F7=E9=E1=EA=DE =F4=EF=F5, =EC=E5 =
=EF=F1=DF=E6=EF=ED=F4=E1 =EF=EB=EF=EA=EB=DE=F1=F9=F3=E7=F2 =F4=E7=F2 =
=F0=F4=F5=F7=E9=E1=EA=DE=F2 =F4=EF=ED<BR>
&gt; =C9=EF=FD=ED=E9=EF-=C9=EF=FD=EB=E9=EF.<BR>
&gt;<BR>
&gt; =CA=E1=EB=DC =D7=F1=E9=F3=F4=EF=FD=E3=E5=ED=ED=E1!<BR>
&gt;<BR>
&gt; =BA=F9=ED<BR>
&gt;<BR>
&gt; ------<BR>
&gt;<BR>
&gt;<BR>
&gt; Dear Tom,<BR>
&gt;<BR>
&gt; Below is a summary of the comments made by the reviewers and a =
summary<BR>
&gt; of the changes and additional work we intend to do to improve =
the<BR>
&gt; article &quot;Learning to Filter Unsolicited Commercial =
E-Mail&quot;. We would be<BR>
&gt; grateful if you could indicate whether or not we are on the right =
track.<BR>
&gt; You may wish to go directly to section 2, where we discuss our plan =
of<BR>
&gt; work.<BR>
&gt;<BR>
&gt; Best regards,<BR>
&gt;<BR>
&gt; Ion<BR>
&gt; --<BR>
&gt; Ion Androutsopoulos -- <A =
HREF=3D"http://www.aueb.gr/users/ion/">http://www.aueb.gr/users/ion/</A><=
BR>
&gt; Lecturer, Dept. of Informatics, Athens Univ. of Economics and =
Business<BR>
&gt;<BR>
&gt; =3D=3D=3D=3D=3D=3D Section 1: Index of points made by the editor =
and the reviewers:<BR>
&gt;<BR>
&gt; The detailed comments of the editor and the reviewers that =
correspond to<BR>
&gt; the labels in square brackets can be found, if needed, in Section =
3.<BR>
&gt;<BR>
&gt; --- Editor:<BR>
&gt;<BR>
&gt; [Ed.1: Corpus too small leading to inadequate evaluation]<BR>
&gt; [Ed.2: Cross-validation problematic, on-line evaluation needed]<BR>
&gt; [Ed.3: Unreliable timings due to slow implementations]<BR>
&gt; [Ed.4: Paper size]<BR>
&gt;<BR>
&gt; --- Reviewer #1:<BR>
&gt;<BR>
&gt; -- Main points:<BR>
&gt;<BR>
&gt; [R1.1: Simplistic corpora]<BR>
&gt; [R1.2: Too few attributes]<BR>
&gt; [R1.3: Unreliable timings due to slow implementations]<BR>
&gt; [R1.4: Poor references to spam side of research]<BR>
&gt; [R1.5: Paper size]<BR>
&gt; [R1.6: Boolean vs. continuous attributes]<BR>
&gt; [R1.7: Cross-validation problematic; on-line evaluation needed]<BR>
&gt; [R1.8: Make classifiers cost-sensitive by setting thresholds =
via<BR>
&gt; cross-validation]<BR>
&gt; [R1.9: More boosting iterations]<BR>
&gt; [R1.10: Real-use study should be on recent data]<BR>
&gt;<BR>
&gt; -- Minor points:<BR>
&gt;<BR>
&gt; [R1.11: More info on Flexible Bayes, probably in the tech. =
report]<BR>
&gt; [R1.12: NB and quadratic discriminants]<BR>
&gt;<BR>
&gt; --- Reviewer #2:<BR>
&gt;<BR>
&gt; [R2.1: Paper size]<BR>
&gt; [R2.2: Corpus size]<BR>
&gt; [R2.3: Evaluate via ROC curves]<BR>
&gt; [R2.4: Set cost-sensitivity via ROC curves]<BR>
&gt; [R2.5: Poor references to spam side of research]<BR>
&gt; [R2.6: Address spam-specific characteristics]<BR>
&gt;<BR>
&gt; --- Reviewer #3:<BR>
&gt;<BR>
&gt; -- Main points:<BR>
&gt;<BR>
&gt; [R3.1: Corpus size and statistical significance]<BR>
&gt; [R3.2: AdaBoost missing]<BR>
&gt; [R3.3: Better analysis via better understanding of algorithms]<BR>
&gt; [R3.4: More users needed in real-life study]<BR>
&gt; [R3.5: Use on-line learning and evaluation]<BR>
&gt; [R3.6: Faster implementations; complexities can be lowered]<BR>
&gt; [R3.7: Paper size]<BR>
&gt;<BR>
&gt; -- Minor points:<BR>
&gt;<BR>
&gt; [R3.8: Points on search bias]<BR>
&gt;<BR>
&gt; =3D=3D=3D=3D=3D=3D Section 2: Changes and additional work we intend =
to do per point:<BR>
&gt;<BR>
&gt; [Ed.4: Paper size]<BR>
&gt; [R1.5: Paper size]<BR>
&gt; [R2.1: Paper size]<BR>
&gt; [R3.7: Paper size]<BR>
&gt;<BR>
&gt; We will remove the detailed presentation of the learning algorithms =
of<BR>
&gt; section 3, and replace it by much briefer descriptions and =
relevant<BR>
&gt; references. Other recommendations made by the reviewers to shorten =
the<BR>
&gt; paper further will also be followed.<BR>
&gt;<BR>
&gt; [Ed.2: Cross-validation problematic, on-line evaluation needed]<BR>
&gt; [R1.7: Cross-validation problematic; on-line evaluation needed]<BR>
&gt; [R3.5: Use on-line learning and evaluation]<BR>
&gt;<BR>
&gt; We will develop new benchmark collections, whose messages will be =
sorted<BR>
&gt; chronologically, in the order they were received. We will use the =
new<BR>
&gt; benchmark collections to perform a version of on-line evaluation: =
with<BR>
&gt; each benchmark collection, we will feed the filters with the =
messages of<BR>
&gt; the collection in the order they arrived, and retrain the filters =
every<BR>
&gt; n (e.g., 100) input messages on all the messages that the filter =
has<BR>
&gt; seen up to that point; between each pair of training points, a =
testing<BR>
&gt; phase will take place, to monitor how well the filter performs on =
the<BR>
&gt; messages that have been received since the last training. We =
believe<BR>
&gt; that this is an adequate modelling of how a learning-based filter =
would<BR>
&gt; be used in practice, which also allows batch learning algorithms to =
be<BR>
&gt; evaluated in an on-line-like manner. Cross-validation will be =
dropped.<BR>
&gt;<BR>
&gt; [Ed.1: Corpus too small leading to inadequate evaluation]<BR>
&gt; [R1.1: Simplistic corpora]<BR>
&gt; [R2.2: Corpus size and statistical validity]<BR>
&gt; [R3.1: Corpus size and statistical significance]<BR>
&gt;<BR>
&gt; As hinted by the second reviewer, &quot;larger corpora&quot; may =
mean two<BR>
&gt; different things: using larger mailboxes (more messages per user), =
or<BR>
&gt; using more mailboxes from different users (messages from more =
users). We<BR>
&gt; plan to use mailboxes from 10-15 different users (as opposed to the =
4<BR>
&gt; users of the current version of the paper), and we will create =
2-3<BR>
&gt; different versions of each mailbox, by modifying the =
legitimate-to-spam<BR>
&gt; proportion, to emulate scenarios where users receive more or fewer =
spam<BR>
&gt; messages. This should help make our results more convincing from =
a<BR>
&gt; statistical point of view; we will also consider statistical<BR>
&gt; significance tests. Regarding the size of the mailboxes, we note =
that at<BR>
&gt; least PU2 and PU3 are not really small; the legitimate messages in =
these<BR>
&gt; collections are all the legitimate messages (6207 and 8824,<BR>
&gt; respectively) that the corresponding users had saved over several =
years.<BR>
&gt; In practice, most users will have saved much fewer legitimate =
messages,<BR>
&gt; so we don't think it is reasonable to experiment with much =
larger<BR>
&gt; mailboxes, nor is it feasible to obtain much larger mailboxes; =
reviewer<BR>
&gt; 3 seems to acknowledge at least some of this points. What makes =
the<BR>
&gt; current mailboxes look small is the white-list emulation that we =
use,<BR>
&gt; which in effect classifies messages from frequent correspondents =
as<BR>
&gt; legitimate without invoking the learning-based components of the =
filters<BR>
&gt; (in PU2 and PU3, this leaves 579+142 and 2313+1826 =
legitimate+spam<BR>
&gt; messages for the learning components to consider). We plan to make =
this<BR>
&gt; clearer in the new version of the paper, and include in the =
evaluation<BR>
&gt; all the messages of the mailboxes, without ignoring in the =
results<BR>
&gt; messages from frequent correspondents. We will report both results =
of<BR>
&gt; filters that use only the white-list and results of filters that =
use<BR>
&gt; both the white-list and the learning algorithms, to show the effect =
of<BR>
&gt; learning. We will also not remove duplicate messages; it's not =
needed<BR>
&gt; since we won't be using cross-validation. This will also help =
increase<BR>
&gt; the size of our mailboxes.<BR>
&gt;<BR>
&gt; [Ed.3: Unreliable timings due to slow implementations]<BR>
&gt; [R1.3: Unreliable timings due to slow implementations]<BR>
&gt; [R3.6: Faster implementations; complexities can be lowered]<BR>
&gt;<BR>
&gt; We'll remove the timing results, as suggested by reviewer 1, and =
point<BR>
&gt; out that timings can vary a lot, depending on the quality of =
the<BR>
&gt; implementations. We'll also explore the possibility of using =
better<BR>
&gt; implementations of the learning algorithms, mostly to help us =
explore<BR>
&gt; larger attribute sets; see point [R1.2: Too few attributes]. In the =
new<BR>
&gt; version of the paper, we'll also mention that the complexity =
figures we<BR>
&gt; show can be improved further, as suggested by reviewer 3.<BR>
&gt;<BR>
&gt; [R3.2: AdaBoost missing]<BR>
&gt; [R1.9: More boosting iterations]<BR>
&gt;<BR>
&gt; We'll replace LogitBoost with AdaBoost, which is similar but =
more<BR>
&gt; well-known. We hope that by using better implementations of the =
training<BR>
&gt; algorithms we will also be able to perform more boosting =
iterations.<BR>
&gt;<BR>
&gt; [R1.6: Boolean vs. continuous attributes]<BR>
&gt;<BR>
&gt; We plan to conduct seperate experiments for Boolean and =
frequency<BR>
&gt; (continuous) attributes. The experiments in the current version of =
the<BR>
&gt; paper were all conducted with frequency attributes.<BR>
&gt;<BR>
&gt; [R1.2: Too few attributes]<BR>
&gt;<BR>
&gt; We'll aim at evaluating all the learning algorithms with up to at =
least<BR>
&gt; 3000 attributes, as suggested by reviewer 3, and possibly more,<BR>
&gt; depending on the efficiency of the implementations we'll employ. =
We<BR>
&gt; note, however, that our current SVM experiments showed no =
significant<BR>
&gt; sign of improvement when moving from 600 to 3000 attributes. The =
SVM<BR>
&gt; configuration that uses n-grams was also tested with up to 3000<BR>
&gt; attributes, and showed no sign of improvement; so, it does not seem =
to<BR>
&gt; be the case that n-grams help when large numbers of attributes are =
used,<BR>
&gt; unlike what reviewer 3 suggested, at least not up to 3000 =
attributes.<BR>
&gt;<BR>
&gt; [R1.8: Make classifiers cost-sensitive by setting thresholds =
via<BR>
&gt; cross-validation]<BR>
&gt; [R2.3: Evaluate via ROC curves]<BR>
&gt; [R2.4: Set cost-sensitivity via ROC curves]<BR>
&gt;<BR>
&gt; We plan to abolish the current fixed cost scenarios (specific =
lambda<BR>
&gt; values), and include in the new version of the paper ROC curves, =
as<BR>
&gt; suggested by reviewer 2. The ROC curves will be formed by =
re-running the<BR>
&gt; experiments with different thresholds for the confidence scores of =
the<BR>
&gt; classifiers. This way we will not have to select specific =
thresholds.&nbsp;<BR>
&gt;<BR>
&gt; [R1.4: Poor references to spam side of research]<BR>
&gt; [R2.5: Poor references to spam side of research]<BR>
&gt;<BR>
&gt; We'll study more recent papers from the spam side of research, =
mostly<BR>
&gt; CEAS papers, and include relevant points in the new version of =
the<BR>
&gt; paper.<BR>
&gt;<BR>
&gt; [R1.10: Real-use study should be on recent data]<BR>
&gt; [R3.4: More users needed in real-life study]<BR>
&gt;<BR>
&gt; The real-use study will be repeated anew. We'll aim at including =
at<BR>
&gt; least five users, as opposed to the one user of the study in the =
current<BR>
&gt; version of the paper. (To answer a question by reviewer 1, the =
real-use<BR>
&gt; study of the current version of the paper, was performed in late =
2003<BR>
&gt; and early 2004. We'll indicate precisely the timespan of the new =
study.)<BR>
&gt;<BR>
&gt; [R2.6: Address spam-specific characteristics]<BR>
&gt;<BR>
&gt; As noted above, we'll use a form of on-line training and evaluation =
that<BR>
&gt; is well-suited to anti-spam filtering, and ROC curves to cope with =
the<BR>
&gt; uncertain misclassification costs of anti-spam filtering. =
Furthermore,<BR>
&gt; we plan to include spam-specific attributes, such as the number =
of<BR>
&gt; attachments, HTML tags, the size of the message, and other =
attributes<BR>
&gt; suggested by researchers in the anti-spam literature.&nbsp;<BR>
&gt;<BR>
&gt; [R1.12: NB and quadratic discriminants]<BR>
&gt; [R3.8: Points on search bias]<BR>
&gt;<BR>
&gt; These will be addressed in the new version of the paper.<BR>
&gt;<BR>
&gt; [R1.11: More info on Flexible Bayes, probably in the tech. =
report]<BR>
&gt;<BR>
&gt; The current version of the paper is already available as a =
technical<BR>
&gt; report from <A =
HREF=3D"http://www.aueb.gr/users/ion/docs/TR2004_updated.pdf">http://www.=
aueb.gr/users/ion/docs/TR2004_updated.pdf</A> . We<BR>
&gt; will consider adding more information on Flexible Bayes in a =
future<BR>
&gt; version of the technical report, as suggested by the first =
reviewer.<BR>
&gt;<BR>
&gt; [R3.3: Better analysis via better understanding of algorithms]<BR>
&gt;<BR>
&gt; We hope to address this point in the new version of the paper, =
though<BR>
&gt; the point was not entirely clear to us.<BR>
&gt;<BR>
&gt; =3D=3D=3D=3D=3D=3D Section 3: Detailed comments made by the editor =
and the<BR>
&gt; reviewers:<BR>
&gt;<BR>
&gt; --- Editor:<BR>
&gt;<BR>
&gt; [Ed.1: Corpus too small leading to inadequate evaluation]<BR>
&gt;<BR>
&gt;&gt;The chief criticism is that your corpus is too small and the<BR>
&gt;&gt;evaluation is inadequate.&nbsp; I agree with the reviewers --- =
your<BR>
&gt;&gt;empirical evaluation isn't really up to the standards of a =
Machine<BR>
&gt;&gt;Learning journal article, in part because the data are too =
limited. On<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;the other hand, I know the difficulty of getting large amounts =
of spam<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;and legitimate email, so I'm sympathetic to your =
situation.&nbsp; I don't<BR>
&gt;&gt;know how much data are available for your evaluation, but I =
encourage<BR>
&gt;&gt;you to consider it.&nbsp; I am willing to lower the evaluation =
standards<BR>
&gt;&gt;in order to publish a significant paper such as this, but<BR>
&gt;&gt;you'll have to improve the size of the data set somewhat.<BR>
&gt;<BR>
&gt;<BR>
&gt; [Ed.2: Cross-validation problematic; on-line evaluation needed]<BR>
&gt;<BR>
&gt;&gt;- All reviewers criticised the fact that you used batch =
evaluation<BR>
&gt;&gt;&nbsp; (ie, cross-validation over an entire corpus) when an =
on-line<BR>
&gt;&gt;&nbsp; evaluation would have been more appropriate for this =
task.&nbsp; I agree<BR>
&gt;&gt;&nbsp; with this -- I think an on-line evaluation would be more =
realistic.<BR>
&gt;&gt;--<BR>
&gt;&gt;&nbsp; Reviewer #3 also suggested looking at on-line learning =
algorithms<BR>
&gt;&gt;&nbsp; for the same reason.<BR>
&gt;<BR>
&gt;<BR>
&gt; [Ed.3: Unreliable timings due to slow implementations]<BR>
&gt;<BR>
&gt;&gt;- Reviewers mentioned that the implementations of the algorithms =
you<BR>
&gt;&gt;&nbsp; chose were very slow, and your timings were therefore =
unreliable.<BR>
&gt;<BR>
&gt;<BR>
&gt; [Ed.4: Paper size]<BR>
&gt;<BR>
&gt;&gt;- All three of the reviewers suggested ways to reduce the =
paper<BR>
&gt;&gt;&nbsp; length; all mentioned that section 3, reviewing the ML =
methods,<BR>
&gt;&gt;&nbsp; could be cut considerably.&nbsp; This seems reasonable =
for an MLJ<BR>
&gt;&gt;&nbsp; article, and I recall that you suggested this =
initially.&nbsp; Based on<BR>
&gt;&gt;&nbsp; the reviewers comments, I think shortening the paper =
would be easy.<BR>
&gt;<BR>
&gt;<BR>
&gt; --- Reviewer 1:<BR>
&gt;<BR>
&gt; [R1.1: Simplistic corpora]<BR>
&gt;<BR>
&gt;&gt;Unfortunately, the corpora used for these experiments are =
relatively<BR>
&gt;&gt;simplistic<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.2: Too few attributes]<BR>
&gt;<BR>
&gt;&gt;and the algorithms are mostly tested<BR>
&gt;&gt;with too few attributes.<BR>
&gt;&gt;--<BR>
&gt;&gt;For instance, part of the reason that at most 600 attributes<BR>
&gt;&gt;are used is because of the speed problems.<BR>
&gt;&gt;--<BR>
&gt;&gt;There doesn't seem to be much impact of training corpus size<BR>
&gt;&gt;on accuracy, but this is in part because you typically use =
so<BR>
&gt;&gt;few attributes (600 or 700) when thousands help =
performance.&nbsp;<BR>
&gt;&gt;Similarly, n-grams don't help, but perhaps this is because<BR>
&gt;&gt;with only 600 attributes, the n-grams kick out equally good<BR>
&gt;&gt;1-grams.&nbsp; What happens with thousands of attributes?<BR>
&gt;&gt;--<BR>
&gt;&gt;The SVM improves substantially with 3000 attributes.&nbsp; =
What<BR>
&gt;&gt;about the other models?&nbsp; It should be possible to run =
all<BR>
&gt;&gt;experiments, except maybe logit-boost, with 3000 =
attributes.&nbsp;<BR>
&gt;&gt;Would this change the relative results?&nbsp;<BR>
&gt;&gt;--<BR>
&gt;&gt;Minimally, all<BR>
&gt;&gt;experiments (except perhaps the boosting experiments) would<BR>
&gt;&gt;be repeated with 3000 attributes.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.3: Unreliable timings due to slow implementations]<BR>
&gt;<BR>
&gt;&gt;All of the timing information<BR>
&gt;&gt;appears to use extremely poor implementations of algorithms.<BR>
&gt;&gt;--<BR>
&gt;&gt;The training and test timing numbers are all highly suspect. =
It<BR>
&gt;&gt;appears that incredibly poor implementations have been =
used.&nbsp; In<BR>
&gt;&gt;particular, in figure 5, why do the test times increase so =
drastically<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;as the number of attributes increases?&nbsp; A good =
implementation of Naive<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;Bayes or SVMs using hash tables should have essentially constant =
time<BR>
&gt;&gt;independent of the number of attributes at test time.&nbsp; Why =
is<BR>
&gt;&gt;Naive Bayes so much slower than the SVM?&nbsp; They are both<BR>
&gt;&gt;linear separators and at test time should both take the same<BR>
&gt;&gt;amount of time.&nbsp; Why does it take up to a second per =
message<BR>
&gt;&gt;to run Naive Bayes? I honestly could not think of a =
technique<BR>
&gt;&gt;that could run this slowly.&nbsp; I consulted a colleague, and =
he<BR>
&gt;&gt;came up with a particularly poor implementation (scan the<BR>
&gt;&gt;message repeatedly, once for each word), that if implemented<BR>
&gt;&gt;slowly might be this slow.<BR>
&gt;&gt;--<BR>
&gt;&gt;I'd recommend excising almost all discussion of speed and<BR>
&gt;&gt;simply noting that Wekka implementations appear to be very<BR>
&gt;&gt;slow and inconsistent, and that is why you won't be reporting =
timings.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.4: Poor references to spam side of research]<BR>
&gt;<BR>
&gt;&gt;The paper does a good job of citing the relevant machine =
learning<BR>
&gt;&gt;research, but a poor job of citing the relevant spam =
research.<BR>
&gt;&gt;--<BR>
&gt;&gt;There is not enough discussion of other published research =
on<BR>
&gt;&gt;spam filtering, especially recent work.&nbsp; There is only =
1<BR>
&gt;&gt;paper about spam filtering dating after 2001 (except for =
self<BR>
&gt;&gt;citations.)&nbsp; Much of the recent research is highly =
relevant.&nbsp;<BR>
&gt;&gt;For instance, there has been recent work on the<BR>
&gt;&gt;susceptibility of machine learning spam filters to good word<BR>
&gt;&gt;attacks.&nbsp; This must be fixed.&nbsp; Good resources include =
Tom<BR>
&gt;&gt;Fawcett's survey paper, and papers from the recent =
Conference<BR>
&gt;&gt;on Email and Anti-Spam (which at least one of the authors =
attended.)<BR>
&gt;&gt;--<BR>
&gt;&gt;In general, there is little discussion of the weaknesses of<BR>
&gt;&gt;machine learning spam filters, and the difficulties of<BR>
&gt;&gt;machine learning for spam filtering.&nbsp; (See, e.g. Tom<BR>
&gt;&gt;Fawcett's paper.)&nbsp; For instance, as soon as a technique<BR>
&gt;&gt;becomes popular, spammers will try to reverse engineer the<BR>
&gt;&gt;technique and work around it.&nbsp; Linear models are =
particularly<BR>
&gt;&gt;susceptible to these attacks.&nbsp; Even personalized models =
may<BR>
&gt;&gt;be susceptible.&nbsp;<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.5: Paper size]<BR>
&gt;<BR>
&gt;&gt;The writing is good, but contains much too much introductory =
material.<BR>
&gt;&gt;It is aimed at a newcomer to machine learning, rather than a =
typical<BR>
&gt;&gt;reader of MLJ.&nbsp; Because of this, the paper is much too =
long.&nbsp; Other<BR>
&gt;&gt;aspects -- clarity, organization, exposition, etc. are all very =
good.<BR>
&gt;&gt;--<BR>
&gt;&gt;(Specific recommendations for shortening the paper also =
given.)<BR>
&gt;&gt;--<BR>
&gt;&gt;Major: The paper must be shortened substantially.&nbsp;<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.6: Boolean vs. continuous attributes]<BR>
&gt;<BR>
&gt;&gt; That said, there is one important part that is left<BR>
&gt;&gt;out: some of the techniques use a boolean feature =
representation, and<BR>
&gt;&gt;some of them use a continuous representation.&nbsp; A table =
showing which<BR>
&gt;&gt;uses which would be very helpful.&nbsp; In particular, I was not =
able to<BR>
&gt;&gt;figure out if your SVM representation used boolean or =
continuous<BR>
&gt;&gt;features.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.7: Cross-validation problematic; on-line evaluation needed]<BR>
&gt;<BR>
&gt;&gt;For section 2, it is worth noting that cross validation for =
spam<BR>
&gt;&gt;filtering yields over-optimistic results.&nbsp; For instance, if =
a new<BR>
&gt;&gt;topic enters my mailbox for some reason, in real life, at some =
point<BR>
&gt;&gt;my training data will never have seen that topic.&nbsp; But with =
cross<BR>
&gt;&gt;validation, there will almost always be some instances of that =
topic<BR>
&gt;&gt;in both training and test.&nbsp; Similarly, if spammers adopt a =
new<BR>
&gt;&gt;technique, e.g. random words, in real life, it will take some =
time for<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;the filter to adapt to this new technique but with cross<BR>
&gt;&gt;validation, there will always be some examples of this in<BR>
&gt;&gt;both training and test.&nbsp; Cross validation may be =
necessary<BR>
&gt;&gt;because of the difficult of obtaining large training =
corpora,<BR>
&gt;&gt;but you need to point out its weaknesses.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.8: Make classifiers cost-sensitive by setting thresholds =
via<BR>
&gt; cross-validation]<BR>
&gt;<BR>
&gt;&gt;Section 4: The techniques used for making the classifiers =
cost<BR>
&gt;&gt;sensitive are too simplistic and cast some doubt on the quality =
of the<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;results.&nbsp; There are many problems here.&nbsp; For instance, =
the fact that<BR>
&gt;&gt;for Naive Bayes you did not use cross validation or other =
techniques<BR>
&gt;&gt;to find a threshold setting leading to optimal scores is very =
poor.<BR>
&gt;&gt;Similarly, SVMs return a score, not just a classification, and =
Platt<BR>
&gt;&gt;has shown that this score can be converted to a probability =
and<BR>
&gt;&gt;then used for cost-sensitive thresholding.&nbsp; =
Alternatively,<BR>
&gt;&gt;for a fixed sensitivity, cross validation to find a =
threshold<BR>
&gt;&gt;on the score can be used directly.&nbsp; The actual technique =
you<BR>
&gt;&gt;used, which throws away training data, almost certainly<BR>
&gt;&gt;adversely impacts the SVM substantially.<BR>
&gt;&gt;--<BR>
&gt;&gt;The SVM results<BR>
&gt;&gt;don't improve as much for lambda=3D9 with 3000 features, but<BR>
&gt;&gt;this may be because of the way you made SVMs cost =
sensitive.&nbsp;<BR>
&gt;&gt;--<BR>
&gt;&gt;Part of the<BR>
&gt;&gt;reason that for SVMs, resampling is used instead of putting<BR>
&gt;&gt;each good training instance in 9 times is because of speed<BR>
&gt;&gt;problems.&nbsp;<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.9: More boosting iterations]<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;Part of the reason that the logit boost results<BR>
&gt;&gt;are worse than others have achieved is because fewer iterations =
are<BR>
&gt;&gt;used which is because of speed problems.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.10: Real-use study should be on recent data]<BR>
&gt;<BR>
&gt;&gt;Exactly when was the 7 month period that the authors used this =
filter?<BR>
&gt;&gt;Spam has been changing considerably, especially as machine =
learning<BR>
&gt;&gt;filters have become more common.&nbsp; If it was in 2002, that =
is much less<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;interesting than if it was in 2004.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R1.11: More info on Flexible Bayes, probably in the tech. =
report]<BR>
&gt;<BR>
&gt;&gt;All that said, I wouldn't mind a little more information =
about<BR>
&gt;&gt;Flexible Bayes (a rarely used technique): in particular, what =
is<BR>
&gt;&gt;Parzen estimation?&nbsp; (It may or may not be appropriate for =
the MLJ<BR>
&gt;&gt;version of the paper, but I'd love to see it discussed in the =
extended<BR>
&gt;<BR>
&gt;<BR>
&gt;&gt;version.)<BR>
&gt;<BR>
&gt;&nbsp;<BR>
&gt; [R1.12: NB and quadratic discriminants]<BR>
&gt;<BR>
&gt;&gt;Section 3.6.1: why do you say that Naive Bayes can learn =
quadratic<BR>
&gt;&gt;discriminants, especially since in section 3.2 you say it can =
only<BR>
&gt;&gt;learn linear discriminants?<BR>
&gt;<BR>
&gt;<BR>
&gt; --- Reviewer 2:<BR>
&gt;<BR>
&gt; [R2.1: Paper size]<BR>
&gt;<BR>
&gt;&gt;They develop in detail the specific<BR>
&gt;&gt;characteristics of each of the approaches (unnecessarily since =
these<BR>
&gt;&gt;details have been well researched previously.)<BR>
&gt;&gt;--<BR>
&gt;&gt;This paper could have been shortened by 50-60% if the =
authors<BR>
&gt;&gt;had provided only the most cursory of summaries of the cited<BR>
&gt;&gt;work (and uncited work well known in the spam filtering<BR>
&gt;&gt;community) and focus on the primary results.<BR>
&gt;&gt;--<BR>
&gt;&gt;3. Drastically reduce the amount of detail used to describe<BR>
&gt;&gt;individual well known techniques (unless novel algorithm<BR>
&gt;&gt;variants are being proposed.)<BR>
&gt;<BR>
&gt;<BR>
&gt; [R2.2: Corpus size]<BR>
&gt;<BR>
&gt;&gt;They apply the approaches to a small well considered&nbsp; =
collection of<BR>
&gt;&gt;data sets and go into deep detail about the&nbsp; data and =
results on that<BR>
&gt;&gt;data.<BR>
&gt;&gt;--<BR>
&gt;&gt;In order to differentiate<BR>
&gt;&gt;classifiers at low false positive rates they need larger =
sets<BR>
&gt;&gt;of valid email (their largest is 2313) or many more sets to<BR>
&gt;&gt;give more statistical validity to their results.<BR>
&gt;&gt;--<BR>
&gt;&gt;In summary, not enough data, not diverse enough data (...<BR>
&gt;&gt;--<BR>
&gt;&gt;2. Use large sources of current evaluation data -- minimum<BR>
&gt;&gt;should be tens of thousands of both valid and spam email =
messages.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R2.3: Evaluate via ROC curves]<BR>
&gt;<BR>
&gt;&gt;The support is inadequate because they don't use ROC graphs =
to<BR>
&gt;&gt;evaluate false positive rates versus true positive rates =
directly.<BR>
&gt;&gt;This is important because there's a very narrow range of =
false<BR>
&gt;&gt;positive rates (e.g. less than 0.1-0.25%) for which a filter =
is<BR>
&gt;&gt;practical.<BR>
&gt;&gt;--<BR>
&gt;&gt;(In summary...), incorrect choice of evaluation method<BR>
&gt;&gt;leading to unclear (and unsummarized accuracy comparisons.)<BR>
&gt;&gt;--<BR>
&gt;&gt;The twenty accuracy graphs<BR>
&gt;&gt;are very difficult to sort out and aren't self contained in<BR>
&gt;&gt;any way. Unfortunately, because of the choice of evalution<BR>
&gt;&gt;(weighted accuracy) they can't easily be used to compare =
with<BR>
&gt;&gt;the plethora of techniques and approaches described in much<BR>
&gt;&gt;of the other work available on spam filtering.<BR>
&gt;&gt;--<BR>
&gt;&gt;3. Change evaluation method to ROC graphs and focus on<BR>
&gt;&gt;regions of low false positive rate to make comparisons.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R2.4: Set cost-sensitivity via ROC curves]<BR>
&gt;<BR>
&gt;&gt;They also<BR>
&gt;&gt;spend a lot of discussion on the cost sensitive nature of =
the<BR>
&gt;&gt;classification but careful use of ROC graphs would allow readers =
to<BR>
&gt;&gt;choose their own points of sensistivity in evaluating the =
results.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R2.5: Poor references to spam side of research]<BR>
&gt;<BR>
&gt;&gt;With respect to the machine learning side they have =
reasonable<BR>
&gt;&gt;coverage of related research but they are very light on the spam =
side<BR>
&gt;&gt;(admittedly smaller body of research). Several important papers =
in<BR>
&gt;&gt;spam filtering such as Fawcett<BR>
&gt;&gt;(2003) &quot;In Vivo Spam Filtering&quot; and Hulten (2004) =
Filtering<BR>
&gt;&gt;spam e-mail on a global scale-- as well as Goodman's<BR>
&gt;&gt;bibliography on Spam (linked at<BR>
&gt;&gt;<A =
HREF=3D"http://research.microsoft.com/~joshuago/">http://research.microso=
ft.com/~joshuago/</A>)<BR>
&gt;&gt;--<BR>
&gt;&gt;Relate and validate those (ROC results to those) reported in =
the<BR>
&gt;<BR>
&gt; various<BR>
&gt;<BR>
&gt;&gt;proceeddings articles -- lists of which are available at the<BR>
&gt;&gt;www.ceas.cc site.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R2.6: Address spam-specific characteristics]<BR>
&gt;<BR>
&gt;&gt;If the authors want to make a contribution to spam filtering =
they need<BR>
&gt;&gt;to thoroughly embed their evaluation in the spam problem as =
there are<BR>
&gt;&gt;a lot of characteristics (adversarial, time varying, =
uncertain<BR>
&gt;&gt;classification costs, high degree of misclassification in =
training<BR>
&gt;&gt;sets, widespread use of &quot;convenience&quot; data, etc.) that =
make the<BR>
&gt;&gt;problem different than those of text classification.<BR>
&gt;&gt;--<BR>
&gt;&gt;1. Incorporate major characteristics and current evaluation<BR>
&gt;&gt;results into comparisons so that the work can be used<BR>
&gt;&gt;generally within the eclectic spam filtering research =
community.<BR>
&gt;<BR>
&gt;<BR>
&gt; --- Reviewer 3:<BR>
&gt;<BR>
&gt; [R3.1: Corpus size and statistical significance]<BR>
&gt;<BR>
&gt;&gt;The corpus is too small, which is<BR>
&gt;&gt;understandable and common for research on spam filtering.<BR>
&gt;&gt;--<BR>
&gt;&gt;Data provided is good for the research community.<BR>
&gt;&gt;-<BR>
&gt;&gt;The PU1, Pu2, PU3, PUA from only 4 users would usually be<BR>
&gt;&gt;considered as a very small corpus for text classification<BR>
&gt;&gt;task in information retrieval community. It would be helpful<BR>
&gt;&gt;if the author can provide some statistical significance<BR>
&gt;&gt;analysis. Analysis in page 36 suggests some conclusions in<BR>
&gt;&gt;other places of the paper may be the results of overfiting the =
data.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R3.2: AdaBoost missing]<BR>
&gt;<BR>
&gt;&gt;The author's coverage of the algorithms are not thorough, =
missing some<BR>
&gt;&gt;very promising algorithms suggested by previous =
publications.<BR>
&gt;&gt;--<BR>
&gt;&gt;1) Adding other learning algorithms such as Adaboost<BR>
&gt;&gt;--<BR>
&gt;&gt;The paper could be stronger if it includes Adaboost<BR>
&gt;&gt;algorithm, since this algorithm works better than any of the<BR>
&gt;&gt;four algorithms tested in the paper on PU1 data set =
(Carreras<BR>
&gt;&gt;and Marquez [2001]).<BR>
&gt;<BR>
&gt;<BR>
&gt; [R3.3: Better analysis via better understanding of algorithms]<BR>
&gt;<BR>
&gt;&gt;Some analysis of the experimental results could be improved =
with<BR>
&gt;&gt;better understanding of the machine learning algorithms.<BR>
&gt;&gt;--<BR>
&gt;&gt;The theory analysis is not strong.<BR>
&gt;&gt;--<BR>
&gt;&gt;The weakness and reasons for observed results are not<BR>
&gt;&gt;explored well in section 5.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R3.4: More users needed in real-life study]<BR>
&gt;<BR>
&gt;&gt;The real user study is useful, although not significant and =
convincing<BR>
&gt;&gt;considering the fact that there is only one user.<BR>
&gt;<BR>
&gt;<BR>
&gt; [R3.5: Use on-line learning and evaluation]<BR>
&gt;<BR>
&gt;&gt;2) Using online learning instead of batch learning, since =
online<BR>
&gt;&gt;learning is a more reasonable/better user scenario for spam =
filtering<BR>
&gt;&gt;--<BR>
&gt;&gt;Figure 10: the results would be more informative if the =
paper<BR>
&gt;&gt;could provide the performance for 1% training data, 2%<BR>
&gt;&gt;training data, 5% training data. For personalized news<BR>
&gt;&gt;filtering, 1% (about 6-10 training<BR>
&gt;&gt;data) is a more reasonable for user to provide initially.<BR>
&gt;&gt;--<BR>
&gt;&gt;For spam filtering, a more reasonable user scenario would be<BR>
&gt;&gt;training on old data, and test on new data. Instead of<BR>
&gt;&gt;ten-fold cross validation, diving training and testing data<BR>
&gt;&gt;according to time would be more realistic. This could be<BR>
&gt;&gt;another reason for why real user evaluation P43 resutls a<BR>
&gt;&gt;much lower recall than cross-validation results.<BR>
&gt;&gt;--<BR>
&gt;&gt;In section 6, the paper presents a nice failure analysis and<BR>
&gt;&gt;made some valuable suggestions about further improvement in<BR>
&gt;&gt;P44-P45. The paper would be stronger by presenting results =
of<BR>
&gt;&gt;using frequently retrained classifier with encoding<BR>
&gt;&gt;improvement to see whether the suggested improvement really<BR>
&gt;&gt;helps or not. This can be done by simulation using the =
second<BR>
&gt;&gt;author's seven months data and online learning algorithm.<BR>
&gt;&gt;(This is not required if the paper is accepted with minor =
revision)<BR>
&gt;<BR>
&gt;<BR>
&gt; [R3.6: Faster implementations; complexities can be lowered]<BR>
&gt;<BR>
&gt;&gt;3) Better implementation of the learning algorithms that would =
work<BR>
&gt;&gt;much faster for this text classification task<BR>
&gt;&gt;--<BR>
&gt;&gt;The experiments carried out in this paper are largely =
limited<BR>
&gt;&gt;by Weka. Although Weka is a good machine learning software,<BR>
&gt;&gt;the implementation of Weka is not optimized for text =
classification.<BR>
&gt;&gt;Some time complexity conclusion (P24, P29) is draw based on<BR>
&gt;&gt;weka implementation instead of the optimal one could =
implement.<BR>
&gt;&gt;For example, the Naive Bayes training complexity can be<BR>
&gt;&gt;reduced to O(avg_doc_length*doc_Num + number_of_features).<BR>
&gt;&gt;With better data structure and engineering, the time<BR>
&gt;&gt;complexity of the feature selection algorithm can also be =
reduced.<BR>
&gt;&gt;It would be helpful if the paper can make this clearer to the =
reader.<BR>
&gt;&gt;--<BR>
&gt;&gt;Section<BR>
&gt;&gt;5.3 would be better if it is more focused on the topic (as<BR>
&gt;&gt;described in the section title) and does not discuss =
&quot;timing&quot; issues.<BR>
&gt;<BR>
&gt;&nbsp;<BR>
&gt; [R3.7: Paper size]<BR>
&gt;<BR>
&gt;&gt;However, it is too detailed while describing some common =
knowledge for<BR>
&gt;&gt;machine learning community.<BR>
&gt;&gt;--<BR>
&gt;&gt;To help the reader focus on important aspects, I suggest the<BR>
&gt;&gt;author replace lengthy introduction of some basic machine<BR>
&gt;&gt;learning knowledge with short summary and references, and<BR>
&gt;&gt;focus on the author's finding.<BR>
&gt;&gt;I would recommend the authors to eliminate or shorten<BR>
&gt;&gt;significantly the following places:<BR>
&gt;&gt;1) Information gain P12<BR>
&gt;&gt;2) Ten fold cross validation P12<BR>
&gt;&gt;3) P13-P23 Naive Bayes, Flexible Bayes, Support vector =
machines,<BR>
&gt;&gt;logitBosst: I suggest use one or two paragraphs to describe<BR>
&gt;&gt;each algorithm with references, and then compare these =
algorithms.<BR>
&gt;<BR>
&gt;<BR>
&gt; (typos and other minor fixes also listed)<BR>
&gt;<BR>
&gt; [R3.8: Points on search bias]<BR>
&gt;<BR>
&gt;&gt;Search bias is the inductive bias introduced by the search =
strategy.<BR>
&gt;&gt;The paper should analyze the search bias more carefully. For =
example:<BR>
&gt;&gt;I couldn't agree with author in &quot;Naive Bayes has a very =
strong search<BR>
&gt;&gt;bias, as it does not perform any search&quot;. =
&quot;Simpleness&quot; does not mean<BR>
&gt;&gt;&quot;Bias&quot;. I won't agree with section 3.6.2 P23, which =
compares search<BR>
&gt;&gt;bias of different algorithms.<BR>
&gt;<BR>
&gt;<BR>
&gt; =3D=3D=3D=3D=3D=3D<BR>
&gt;<BR>
&gt;<BR>
<BR>
</FONT>
</P>

</BODY>
</HTML>
------=_NextPart_000_184B_01C5B2FD.38349F90--
